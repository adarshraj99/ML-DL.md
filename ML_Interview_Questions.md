##  Common Q:

1. What are the Types of Machine learning ?
2. what is Supervised learning, types, uses ,adv/dis-adv, and some types of each type of supervised learning.
3. What is unsupervised learning, types, uses ,adv/dis-adv, and some types of each type of unsupervised learning.
4. What is Hybrid learning ,types, uses ,adv/dis-adv, and some types of each type of Hybrid learning.
5. What is Reinforcement learning ,types ,uses ,adv/dis-adv, and some types of each type of supervised learning. 
6. Advantages ,disadvantages of ML.
7. Product development and Testing Lifecycle.
8. What are the types of Neural Networks ?
9. What are the types of Testing in ML  and when to do them in workflow ?
11. Regularization and Techniques.
12. Some Famous ML Frameworks and their usecases accouring to model types, adv/disadv.
13. Explain these types of testings: 
* Temprature Testing
* 0 shot Testing
* Chain of thought
* Fantasy Claim Testing
* Accuracy Testing
* Repeatablity Testing
* Style Transfer Testing
* Invariance Testing
* BiDirectional Testing
* Responsible AI testing
* Transperancy Testing
* Ethical Testing
* Data Privacy and Security Testing
* Model Generalization Testing
* Social Impact Testing
* Integration Testing
* Latency Testing
* Drift Testing
* Shadow Testing
* A/B testing
* 
14. Understanding of how LLMs work and how to test them
15. Prompt-based testing scenarios
16. How do you test for the correctness, relevance, and consistency of an LLM response?
17. How would you automate validation of LLM responses against expected outputs?
18. What is prompt injection and how do you test for it?


## API testing (OpenAI or Anthropic APIs):
1. How do you integrate OpenAI’s API in Python? (They may ask you to write sample code)
2. Parameters/headers/auth for response & response for OpenAI API testing.
3. what are the parameters for : openai.ChatCompletion.create()
4. How do you write tests for different temperature or model versions?
5. Write a pytest-based test to call OpenAI API and validate the response.
6. How can you use parameterization in pytest to test multiple prompts?
7. How do you handle flaky test results in LLM testing?
8. How would you compare model responses between OpenAI GPT-4 and Anthropic Claude?
9. Automate the following: Call OpenAI API with different prompts and validate if keywords are present in responses.
10. Test a summarization prompt — validate if the summary is under 3 sentences and includes key facts.
11. Explain your understanding of precision, recall, and how they apply in LLM QA.


## Prompt Engineering:
1. How would you write a test case to validate a prompt that generates SQL queries?
2. How would you detect hallucinations in LLM output?
3. How do you build a prompt set for regression testing?
4. How do you build a prompt set for regression testing?
5. How would you set up regression tests for prompts over time?
6. How do you test if a prompt returns biased, toxic, or unsafe content?
7. How would you score prompt results: Exact match vs semantic similarity?


## Deeper Q:
1. LLM use-case design ,prompt validation strategies
2. Building a small LLM QA framework .ex: Design a framework to test LLM-powered chatbot responses for relevance and tone.
3. Code review / debugging skills
4. ML & data exposure (light, but present)
5. How would you test vector search output from a retrieval-augmented generation (RAG) system?
